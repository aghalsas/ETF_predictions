{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, StackingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import warnings\n",
    "import Feature_Creation\n",
    "from Feature_Creation import create_features\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_frame(target_ETF, horizon = 1):\n",
    "    ##Import dates\n",
    "    frame_10 = pd.read_csv(os.path.join(os.path.abspath(os.getcwd()),'data','10_ETF','SPY.csv'),usecols=['Date'])\n",
    "    # Check if defined horizon is in approved list\n",
    "    horizon_list = [1,2,3,5,10,20,40,60,120,250]\n",
    "    if horizon not in horizon_list:\n",
    "        raise ValueError(\"horizon must be one of [1,2,3,5,10,20,40,60,120,250]\")\n",
    "    \n",
    "    frame_10['Date'] = pd.to_datetime(frame_10['Date'])\n",
    "    frame_10['Month'] = frame_10['Date'].dt.month\n",
    "    frame_10['dayowk'] = frame_10['Date'].dt.dayofweek\n",
    "    frame_10 = pd.get_dummies(data = frame_10,columns = ['Month','dayowk'])\n",
    "    frame_10.drop(['Date'],axis=1,inplace=True)\n",
    "    \n",
    "    ##Joining all the ETF's together\n",
    "    for etf in ['SPY','IWM','EEM','TLT','LQD','TIP','IYR','GLD','OIH','FXE']:\n",
    "        frame = pd.read_csv(os.path.join(os.path.abspath(os.getcwd()),'data','10_ETF',etf+'.csv'),usecols=['Volume','Adj Close'])\n",
    "#        frame.rename(columns={'Volume':etf+'_volume'}, inplace=True)\n",
    "        if horizon == 1:\n",
    "            frame[etf+'_h_ret'] = (frame['Adj Close']/frame['Adj Close'].shift(1)) -1\n",
    "            frame[etf+'volume'] = frame['Volume']\n",
    "        else:\n",
    "            frame[etf+'_h_ret'] = (frame['Adj Close']/frame['Adj Close'].shift(horizon)) -1\n",
    "            lagged =  horizon_list[horizon_list.index(horizon)-1]\n",
    "            for j in range(1,lagged+1):\n",
    "                frame[etf +'_'+ str(j)+'_lag_ret'] = frame[etf+'_h_ret'].shift(j)#(frame['Adj Close'].shift(j)/frame['Adj Close'].shift(horizon+j)) -1\n",
    "            frame[etf+'_h_vol'] = frame['Volume'].rolling(horizon).mean()\n",
    "            for j in range(1,lagged+1):\n",
    "                frame[etf +'_'+ str(j)+'_lag_vol'] = frame[etf+'_h_vol'].shift(j)\n",
    "        if etf==target_ETF:\n",
    "            frame['target'] = frame['Adj Close'] <= frame['Adj Close'].shift(-horizon)\n",
    "        frame.drop(['Adj Close'],axis=1,inplace=True)\n",
    "        frame.drop(['Volume'],axis=1,inplace=True)\n",
    "        frame_10 = pd.concat([frame_10, frame],axis=1) \n",
    "    return frame_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_frame_rand(target_ETF, horizon = 1):\n",
    "    ##Import dates\n",
    "    frame_10 = pd.read_csv(os.path.join(os.path.abspath(os.getcwd()),'data','10_ETF','SPY.csv'),usecols=['Date'])\n",
    "    # Check if defined horizon is in approved list\n",
    "    horizon_list = [1,2,3,5,10,20,40,60,120,250]\n",
    "    if horizon not in horizon_list:\n",
    "        raise ValueError(\"horizon must be one of [1,2,3,5,10,20,40,60,120,250]\")\n",
    "    \n",
    "    frame_10['Date'] = pd.to_datetime(frame_10['Date'])\n",
    "    frame_10['Month'] = frame_10['Date'].dt.month\n",
    "    frame_10['dayowk'] = frame_10['Date'].dt.dayofweek\n",
    "    frame_10 = pd.get_dummies(data = frame_10,columns = ['Month','dayowk'])\n",
    "    frame_10.drop(['Date'],axis=1,inplace=True)\n",
    "    \n",
    "    ##Joining all the ETF's together\n",
    "    for etf in ['random1','random2','random3','random4','random5']:\n",
    "        frame = pd.read_csv(os.path.join(os.path.abspath(os.getcwd()),'data','10_ETF',etf+'.csv'),usecols=['Volume','Adj Close'])\n",
    "#        frame.rename(columns={'Volume':etf+'_volume'}, inplace=True)\n",
    "        if horizon == 1:\n",
    "            frame[etf+'_h_ret'] = (frame['Adj Close']/frame['Adj Close'].shift(1)) -1\n",
    "            frame[etf+'volume'] = frame['Volume']\n",
    "        else:\n",
    "            frame[etf+'_h_ret'] = (frame['Adj Close']/frame['Adj Close'].shift(horizon)) -1\n",
    "            lagged =  horizon_list[horizon_list.index(horizon)-1]\n",
    "            for j in range(1,lagged+1):\n",
    "                frame[etf +'_'+ str(j)+'_lag_ret'] = frame[etf+'_h_ret'].shift(j)#(frame['Adj Close'].shift(j)/frame['Adj Close'].shift(horizon+j)) -1\n",
    "            frame[etf+'_h_vol'] = frame['Volume'].rolling(horizon).mean()\n",
    "            for j in range(1,lagged+1):\n",
    "                frame[etf +'_'+ str(j)+'_lag_vol'] = frame[etf+'_h_vol'].shift(j)\n",
    "        if etf==target_ETF:\n",
    "            frame['target'] = frame['Adj Close'] <= frame['Adj Close'].shift(-horizon)\n",
    "        frame.drop(['Adj Close'],axis=1,inplace=True)\n",
    "        frame.drop(['Volume'],axis=1,inplace=True)\n",
    "        frame_10 = pd.concat([frame_10, frame],axis=1) \n",
    "    return frame_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def liew_mayster(data_feat,CV= 5,verbose=False,do_forest=False,do_rf =True,do_logreg =True,do_svm=True,do_xgb=True,do_stacking=True):\n",
    "    beginning = datetime.datetime.now()\n",
    "    data_feat.dropna(inplace=True)\n",
    "    if verbose==True:\n",
    "        print(data_feat.head())\n",
    "    \n",
    "    \n",
    "    RF_dict = {}\n",
    "    logreg_poly_dict = {}\n",
    "    SVM_poly_dict = {}\n",
    "    XGB_dict = {}\n",
    "    ### Normalizing Features and creating test train split and time series cross-validation\n",
    "    y = data_feat['target'].astype(int)\n",
    "    X = data_feat.drop(['target'],axis=1)\n",
    "    #X.dropna(inplace=True)\n",
    "    \n",
    "    ### Continuous features\n",
    "    continuous = X.columns[X.nunique()>=3]\n",
    "    ### Discrete features\n",
    "    discrete = X.columns[X.nunique()< 3]\n",
    "    ### Scale continuos features\n",
    "    scaler = StandardScaler()\n",
    "    X_cont = pd.DataFrame(scaler.fit_transform(X[continuous]),columns=continuous)\n",
    "    \n",
    "    ### Discerete Features\n",
    "    X_disc = X[discrete]\n",
    "\n",
    "    X_cont.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    X_disc.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    ### Combining\n",
    "    X = pd.concat([X_cont,X_disc],axis=1)\n",
    "\n",
    "    if CV=='tscv':    \n",
    "        train_size = X.shape[0]*4//5\n",
    "        X_test = X.iloc[train_size:]\n",
    "        X_train = X.iloc[0:train_size]\n",
    "        y_test = y.iloc[train_size:]\n",
    "        y_train = y.iloc[0:train_size]\n",
    "\n",
    "        ### Time series cross-validation\n",
    "        tscv = TimeSeriesSplit(n_splits=4)\n",
    "    else:\n",
    "        #print('here')\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    ### Naive Prediction\n",
    "    y_hat_test_naive = np.ones(len(y_test))\n",
    "    naive_dict = {'model':'Naive','precision':precision_score(y_hat_test_naive,y_test),\n",
    "                    'recall':recall_score(y_hat_test_naive,y_test),\n",
    "                    'accuracy':accuracy_score(y_hat_test_naive,y_test),\n",
    "                    'f1':f1_score(y_hat_test_naive,y_test)}\n",
    "                 #'return':np.nansum((np.array(X_test['1day_pct'].shift(-1))*(np.array(y_hat_test_naive))))}\n",
    "    #if verbose==True:\n",
    "     #   print('naive return:',np.nansum((np.array(X_test['1day_pct'].shift(-1))*(np.array(y_hat_test_naive)))))\n",
    "\n",
    "    ### Large Forest\n",
    "    if do_forest == True:\n",
    "            forest = RandomForestClassifier(n_estimators=3000, max_depth= 10)\n",
    "            forest.fit(X_train, y_train)\n",
    "            #plot_feature_importances(forest,n_features=50)\n",
    "    \n",
    "    print(accuracy_score(y_hat_test_naive,y_test))\n",
    "    start = datetime.datetime.now()    \n",
    "    if do_rf == True:\n",
    "        rf_clf = RandomForestClassifier()\n",
    "        rf_param_grid = {\n",
    "            'n_estimators': [100],\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'max_depth': [None, 2, 3, 5,10],\n",
    "            'min_samples_split': [5,10,15],\n",
    "            'min_samples_leaf': [3,5,9,13]\n",
    "        }\n",
    "        rf_grid_search = GridSearchCV(rf_clf, rf_param_grid, cv=CV,n_jobs=-1)\n",
    "        rf_grid_search.fit(X_train, y_train)\n",
    "        if verbose==True:\n",
    "            print(f\"Training Accuracy: {rf_grid_search.best_score_ :.2%}\")\n",
    "            print(\"\")\n",
    "            print(f\"Optimal Parameters: {rf_grid_search.best_params_}\")\n",
    "        best_rf = rf_grid_search.best_params_\n",
    "\n",
    "        y_hat_test_RF = rf_grid_search.predict(X_test)\n",
    "        RF_dict = {'model':'RF','precision':precision_score(y_hat_test_RF,y_test),'recall':recall_score(y_hat_test_RF,y_test),\n",
    "           'accuracy':accuracy_score(y_hat_test_RF,y_test),'f1':f1_score(y_hat_test_RF,y_test),\n",
    "           #'return':np.nansum((np.array(X_test['1day_pct'].shift(-1))*((np.array(y_hat_test_RF)-1/2)*2))),\n",
    "            'information gain':accuracy_score(y_hat_test_RF,y_test)-accuracy_score(y_hat_test_naive,y_test)}\n",
    "        if verbose==True:\n",
    "            print(RF_dict)\n",
    "    end = datetime.datetime.now()\n",
    "    print('RF time', end-start)\n",
    "    \n",
    "    start = datetime.datetime.now()\n",
    "    ### Logistic Regression\n",
    "    if do_logreg == True:\n",
    "        logreg_clf = LogisticRegression()\n",
    "        logreg_param_grid = {\n",
    "            'fit_intercept': [True,False],\n",
    "            'solver':['liblinear'],\n",
    "            'C': np.logspace(0,4,5),\n",
    "            'penalty': ['l2'],\n",
    "        }\n",
    "        logreg_grid_search = GridSearchCV(logreg_clf, logreg_param_grid, cv=CV,n_jobs=-1)\n",
    "        logreg_grid_search.fit(X_train, y_train)\n",
    "        y_hat_test_log = logreg_grid_search.predict(X_test)\n",
    "        logreg_poly_dict = {'model':'Logistic','precision':precision_score(y_hat_test_log,y_test),\n",
    "                    'recall':recall_score(y_hat_test_log,y_test),\n",
    "                   'accuracy':accuracy_score(y_hat_test_log,y_test),\n",
    "                    'f1':f1_score(y_hat_test_log,y_test),\n",
    "                   #'return':np.nansum((np.array(X_test['1day_pct'].shift(-1))*((np.array(y_hat_test_log)-1/2)*2))),\n",
    "                    'information gain':accuracy_score(y_hat_test_log,y_test)-accuracy_score(y_hat_test_naive,y_test)}\n",
    "        if verbose==True:\n",
    "            print(logreg_poly_dict)\n",
    "    end = datetime.datetime.now()    \n",
    "    print('LOG time', end-start)\n",
    "    \n",
    "    start = datetime.datetime.now()\n",
    "    ### SVM poly\n",
    "    if do_svm==True:\n",
    "        svm_clf_poly = svm.SVC(kernel='poly')\n",
    "        r_range =  np.array([0.25,0.5, 1,2,4])\n",
    "        gamma_range =  np.array([0.0001,0.001, 0.01,0.1])\n",
    "        d_range = np.array([2,3, 4])\n",
    "        param_grid = dict(gamma=gamma_range, degree=d_range, coef0=r_range)\n",
    "        svm_grid_search_poly = GridSearchCV(svm_clf_poly, param_grid, cv=CV,n_jobs=-1)\n",
    "        svm_grid_search_poly.fit(X_train, y_train)\n",
    "        best_svm = svm_grid_search_poly.best_params_\n",
    "        y_hat_test_svm_poly = svm_grid_search_poly.predict(X_test)\n",
    "        SVM_poly_dict = {'model':'SVM_poly','precision':precision_score(y_hat_test_svm_poly,y_test),'recall':recall_score(y_hat_test_svm_poly,y_test),\n",
    "               'accuracy':accuracy_score(y_hat_test_svm_poly,y_test),'f1':f1_score(y_hat_test_svm_poly,y_test),\n",
    "                #'return':np.nansum((np.array(X_test['1day_pct'].shift(-1))*((np.array(y_hat_test_svm_poly)-1/2)*2))),\n",
    "                'information gain':accuracy_score(y_hat_test_svm_poly,y_test)-accuracy_score(y_hat_test_naive,y_test)}\n",
    "        if verbose==True:\n",
    "            print(SVM_poly_dict)\n",
    "    end = datetime.datetime.now()\n",
    "    print('SVM time', end-start)\n",
    "    \n",
    "    start = datetime.datetime.now()\n",
    "    ##XGB\n",
    "    if do_xgb==True:\n",
    "        estimator = XGBClassifier(\n",
    "        objective= 'binary:logistic',\n",
    "        nthread=2,\n",
    "        seed=42)\n",
    "        parameters = {\n",
    "            'max_depth': range (2, 10, 2),\n",
    "            'n_estimators': range(20, 120, 20),\n",
    "            'learning_rate': [0.001,0.003,0.01, 0.03, 0.1]\n",
    "        }\n",
    "\n",
    "        xgb_grid_search = GridSearchCV(\n",
    "            estimator=estimator,\n",
    "            param_grid=parameters,\n",
    "            n_jobs = -1,\n",
    "            cv = CV,\n",
    "            verbose=False\n",
    "        )\n",
    "        xgb_grid_search.fit(X_train, y_train)\n",
    "        if verbose==True:\n",
    "            print(f\"Training Accuracy: {xgb_grid_search.best_score_ :.2%}\")\n",
    "            print(\"\")\n",
    "            print(f\"Optimal Parameters: {xgb_grid_search.best_params_}\")\n",
    "        xgb_best = xgb_grid_search.best_params_\n",
    "        y_hat_test_XGB = xgb_grid_search.predict(X_test)\n",
    "        XGB_dict = {'model':'XGB','precision':precision_score(y_hat_test_XGB,y_test),'recall':recall_score(y_hat_test_XGB,y_test),\n",
    "                   'accuracy':accuracy_score(y_hat_test_XGB,y_test),'f1':f1_score(y_hat_test_XGB,y_test),\n",
    "                   #'return':np.nansum((np.array(X_test['1day_pct'].shift(-1))*((np.array(y_hat_test_XGB)-1/2)*2))),\n",
    "                   'information gain':accuracy_score(y_hat_test_XGB,y_test)-accuracy_score(y_hat_test_naive,y_test)}\n",
    "        if verbose==True:\n",
    "            print(XGB_dict)\n",
    "    end = datetime.datetime.now()    \n",
    "    print('XGB time', end-start)\n",
    "    \n",
    "    start = datetime.datetime.now()\n",
    "    if do_stacking==True:\n",
    "        rf_base = RandomForestClassifier(n_estimators = best_rf['n_estimators'],\n",
    "                                               criterion = best_rf['criterion'],max_depth=best_rf['max_depth'],\n",
    "                                               min_samples_split = best_rf['min_samples_split'],\n",
    "                                              min_samples_leaf= best_rf['min_samples_leaf'])\n",
    "        xgb_base = XGBClassifier(n_estimators = xgb_best['n_estimators'],\n",
    "                                 max_depth = xgb_best['max_depth'],\n",
    "                                 learning_rate = xgb_best['learning_rate'],\n",
    "                                objective= 'binary:logistic',\n",
    "                                nthread=2,\n",
    "                                seed=42)\n",
    "        base_models = [('random_forest', rf_base),\n",
    "               ('xgb', xgb_base)]          \n",
    "        stack_clf = StackingClassifier(estimators = base_models,final_estimator = LogisticRegression(),\n",
    "                                           cv = 5)\n",
    "        stack_clf.fit(X_train, y_train)\n",
    "        y_hat_test_stack = stack_clf.predict(X_test)\n",
    "        stack_dict = {'model':'stack','precision':precision_score(y_hat_test_stack,y_test),'recall':recall_score(y_hat_test_stack,y_test),\n",
    "                   'accuracy':accuracy_score(y_hat_test_stack,y_test),'f1':f1_score(y_hat_test_stack,y_test),\n",
    "                   #'return':np.nansum((np.array(X_test['1day_pct'].shift(-1))*((np.array(y_hat_test_stack)-1/2)*2))),\n",
    "                     'information gain':accuracy_score(y_hat_test_stack,y_test)-accuracy_score(y_hat_test_naive,y_test)}\n",
    "        if verbose==True:\n",
    "            print(stack_dict)\n",
    "    end = datetime.datetime.now()\n",
    "    print('stack time', end-start)\n",
    "    print('Total',end-beginning)\n",
    "    return naive_dict, RF_dict, logreg_poly_dict, SVM_poly_dict, XGB_dict, stack_dict\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month_1</th>\n",
       "      <th>Month_2</th>\n",
       "      <th>Month_3</th>\n",
       "      <th>Month_4</th>\n",
       "      <th>Month_5</th>\n",
       "      <th>Month_6</th>\n",
       "      <th>Month_7</th>\n",
       "      <th>Month_8</th>\n",
       "      <th>Month_9</th>\n",
       "      <th>Month_10</th>\n",
       "      <th>...</th>\n",
       "      <th>random4_2_lag_vol</th>\n",
       "      <th>random4_3_lag_vol</th>\n",
       "      <th>random5_h_ret</th>\n",
       "      <th>random5_1_lag_ret</th>\n",
       "      <th>random5_2_lag_ret</th>\n",
       "      <th>random5_3_lag_ret</th>\n",
       "      <th>random5_h_vol</th>\n",
       "      <th>random5_1_lag_vol</th>\n",
       "      <th>random5_2_lag_vol</th>\n",
       "      <th>random5_3_lag_vol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111891480.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>75872840.0</td>\n",
       "      <td>72778780.0</td>\n",
       "      <td>-0.028414</td>\n",
       "      <td>-0.037097</td>\n",
       "      <td>-0.028795</td>\n",
       "      <td>-0.011440</td>\n",
       "      <td>71511540.0</td>\n",
       "      <td>69298620.0</td>\n",
       "      <td>62826260.0</td>\n",
       "      <td>69983160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>70824320.0</td>\n",
       "      <td>75872840.0</td>\n",
       "      <td>-0.009241</td>\n",
       "      <td>-0.028414</td>\n",
       "      <td>-0.037097</td>\n",
       "      <td>-0.028795</td>\n",
       "      <td>97533640.0</td>\n",
       "      <td>71511540.0</td>\n",
       "      <td>69298620.0</td>\n",
       "      <td>62826260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>83415200.0</td>\n",
       "      <td>70824320.0</td>\n",
       "      <td>-0.006570</td>\n",
       "      <td>-0.009241</td>\n",
       "      <td>-0.028414</td>\n",
       "      <td>-0.037097</td>\n",
       "      <td>111501460.0</td>\n",
       "      <td>97533640.0</td>\n",
       "      <td>71511540.0</td>\n",
       "      <td>69298620.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>80976880.0</td>\n",
       "      <td>83415200.0</td>\n",
       "      <td>0.001974</td>\n",
       "      <td>-0.006570</td>\n",
       "      <td>-0.009241</td>\n",
       "      <td>-0.028414</td>\n",
       "      <td>118716520.0</td>\n",
       "      <td>111501460.0</td>\n",
       "      <td>97533640.0</td>\n",
       "      <td>71511540.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>79761620.0</td>\n",
       "      <td>80976880.0</td>\n",
       "      <td>-0.003357</td>\n",
       "      <td>0.001974</td>\n",
       "      <td>-0.006570</td>\n",
       "      <td>-0.009241</td>\n",
       "      <td>111604080.0</td>\n",
       "      <td>118716520.0</td>\n",
       "      <td>111501460.0</td>\n",
       "      <td>97533640.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1258 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Month_1  Month_2  Month_3  Month_4  Month_5  Month_6  Month_7  Month_8  \\\n",
       "0           0        0        0        0        0        0        0        0   \n",
       "1           1        0        0        0        0        0        0        0   \n",
       "2           1        0        0        0        0        0        0        0   \n",
       "3           1        0        0        0        0        0        0        0   \n",
       "4           1        0        0        0        0        0        0        0   \n",
       "...       ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "1253        0        0        0        0        0        0        0        0   \n",
       "1254        0        0        0        0        0        0        0        0   \n",
       "1255        0        0        0        0        0        0        0        0   \n",
       "1256        0        0        0        0        0        0        0        0   \n",
       "1257        0        0        0        0        0        0        0        0   \n",
       "\n",
       "      Month_9  Month_10  ...  random4_2_lag_vol  random4_3_lag_vol  \\\n",
       "0           0         0  ...                NaN                NaN   \n",
       "1           0         0  ...                NaN                NaN   \n",
       "2           0         0  ...                NaN                NaN   \n",
       "3           0         0  ...                NaN                NaN   \n",
       "4           0         0  ...                NaN                NaN   \n",
       "...       ...       ...  ...                ...                ...   \n",
       "1253        0         0  ...         75872840.0         72778780.0   \n",
       "1254        0         0  ...         70824320.0         75872840.0   \n",
       "1255        0         0  ...         83415200.0         70824320.0   \n",
       "1256        0         0  ...         80976880.0         83415200.0   \n",
       "1257        0         0  ...         79761620.0         80976880.0   \n",
       "\n",
       "      random5_h_ret  random5_1_lag_ret  random5_2_lag_ret  random5_3_lag_ret  \\\n",
       "0               NaN                NaN                NaN                NaN   \n",
       "1               NaN                NaN                NaN                NaN   \n",
       "2               NaN                NaN                NaN                NaN   \n",
       "3               NaN                NaN                NaN                NaN   \n",
       "4               NaN                NaN                NaN                NaN   \n",
       "...             ...                ...                ...                ...   \n",
       "1253      -0.028414          -0.037097          -0.028795          -0.011440   \n",
       "1254      -0.009241          -0.028414          -0.037097          -0.028795   \n",
       "1255      -0.006570          -0.009241          -0.028414          -0.037097   \n",
       "1256       0.001974          -0.006570          -0.009241          -0.028414   \n",
       "1257      -0.003357           0.001974          -0.006570          -0.009241   \n",
       "\n",
       "      random5_h_vol  random5_1_lag_vol  random5_2_lag_vol  random5_3_lag_vol  \n",
       "0               NaN                NaN                NaN                NaN  \n",
       "1               NaN                NaN                NaN                NaN  \n",
       "2               NaN                NaN                NaN                NaN  \n",
       "3               NaN                NaN                NaN                NaN  \n",
       "4       111891480.0                NaN                NaN                NaN  \n",
       "...             ...                ...                ...                ...  \n",
       "1253     71511540.0         69298620.0         62826260.0         69983160.0  \n",
       "1254     97533640.0         71511540.0         69298620.0         62826260.0  \n",
       "1255    111501460.0         97533640.0         71511540.0         69298620.0  \n",
       "1256    118716520.0        111501460.0         97533640.0         71511540.0  \n",
       "1257    111604080.0        118716520.0        111501460.0         97533640.0  \n",
       "\n",
       "[1258 rows x 58 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_frame_rand('random1',horizon=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Month_1  Month_2  Month_3  Month_4  Month_5  Month_6  Month_7  Month_8  \\\n",
      "8         1        0        0        0        0        0        0        0   \n",
      "9         1        0        0        0        0        0        0        0   \n",
      "10        1        0        0        0        0        0        0        0   \n",
      "11        1        0        0        0        0        0        0        0   \n",
      "12        1        0        0        0        0        0        0        0   \n",
      "\n",
      "    Month_9  Month_10  ...  random4_2_lag_vol  random4_3_lag_vol  \\\n",
      "8         0         0  ...         72453200.0         92760200.0   \n",
      "9         0         0  ...         99679880.0         72453200.0   \n",
      "10        0         0  ...        114768740.0         99679880.0   \n",
      "11        0         0  ...        116613480.0        114768740.0   \n",
      "12        0         0  ...        126717860.0        116613480.0   \n",
      "\n",
      "    random5_h_ret  random5_1_lag_ret  random5_2_lag_ret  random5_3_lag_ret  \\\n",
      "8       -0.014962          -0.022163          -0.006473          -0.011068   \n",
      "9       -0.003983          -0.014962          -0.022163          -0.006473   \n",
      "10      -0.016406          -0.003983          -0.014962          -0.022163   \n",
      "11      -0.026214          -0.016406          -0.003983          -0.014962   \n",
      "12      -0.033828          -0.026214          -0.016406          -0.003983   \n",
      "\n",
      "    random5_h_vol  random5_1_lag_vol  random5_2_lag_vol  random5_3_lag_vol  \n",
      "8     111121680.0         97655560.0        112042020.0        116726340.0  \n",
      "9      86767640.0        111121680.0         97655560.0        112042020.0  \n",
      "10    106446900.0         86767640.0        111121680.0         97655560.0  \n",
      "11    110679480.0        106446900.0         86767640.0        111121680.0  \n",
      "12    102083320.0        110679480.0        106446900.0         86767640.0  \n",
      "\n",
      "[5 rows x 58 columns]\n",
      "0.552\n",
      "Training Accuracy: 67.90%\n",
      "\n",
      "Optimal Parameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 3, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "{'model': 'RF', 'precision': 0.6231884057971014, 'recall': 0.6417910447761194, 'accuracy': 0.6, 'f1': 0.6323529411764706, 'information gain': 0.04799999999999993}\n",
      "RF time 0:00:24.380109\n",
      "{'model': 'Logistic', 'precision': 0.6159420289855072, 'recall': 0.6439393939393939, 'accuracy': 0.6, 'f1': 0.6296296296296297, 'information gain': 0.04799999999999993}\n",
      "LOG time 0:00:00.123847\n",
      "{'model': 'SVM_poly', 'precision': 0.8043478260869565, 'recall': 0.7762237762237763, 'accuracy': 0.764, 'f1': 0.7900355871886121, 'information gain': 0.21199999999999997}\n",
      "SVM time 0:00:03.263323\n",
      "Training Accuracy: 67.30%\n",
      "\n",
      "Optimal Parameters: {'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 80}\n",
      "{'model': 'XGB', 'precision': 0.6594202898550725, 'recall': 0.6893939393939394, 'accuracy': 0.648, 'f1': 0.6740740740740742, 'information gain': 0.09599999999999997}\n",
      "XGB time 0:00:29.082950\n",
      "{'model': 'stack', 'precision': 0.6739130434782609, 'recall': 0.7045454545454546, 'accuracy': 0.664, 'f1': 0.688888888888889, 'information gain': 0.11199999999999999}\n",
      "stack time 0:00:02.843408\n",
      "Total 0:00:59.712865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'model': 'Naive',\n",
       "  'precision': 1.0,\n",
       "  'recall': 0.552,\n",
       "  'accuracy': 0.552,\n",
       "  'f1': 0.711340206185567},\n",
       " {'model': 'RF',\n",
       "  'precision': 0.6231884057971014,\n",
       "  'recall': 0.6417910447761194,\n",
       "  'accuracy': 0.6,\n",
       "  'f1': 0.6323529411764706,\n",
       "  'information gain': 0.04799999999999993},\n",
       " {'model': 'Logistic',\n",
       "  'precision': 0.6159420289855072,\n",
       "  'recall': 0.6439393939393939,\n",
       "  'accuracy': 0.6,\n",
       "  'f1': 0.6296296296296297,\n",
       "  'information gain': 0.04799999999999993},\n",
       " {'model': 'SVM_poly',\n",
       "  'precision': 0.8043478260869565,\n",
       "  'recall': 0.7762237762237763,\n",
       "  'accuracy': 0.764,\n",
       "  'f1': 0.7900355871886121,\n",
       "  'information gain': 0.21199999999999997},\n",
       " {'model': 'XGB',\n",
       "  'precision': 0.6594202898550725,\n",
       "  'recall': 0.6893939393939394,\n",
       "  'accuracy': 0.648,\n",
       "  'f1': 0.6740740740740742,\n",
       "  'information gain': 0.09599999999999997},\n",
       " {'model': 'stack',\n",
       "  'precision': 0.6739130434782609,\n",
       "  'recall': 0.7045454545454546,\n",
       "  'accuracy': 0.664,\n",
       "  'f1': 0.688888888888889,\n",
       "  'information gain': 0.11199999999999999})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liew_mayster(create_frame_rand('random1',horizon=5),verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
