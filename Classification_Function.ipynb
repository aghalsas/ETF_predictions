{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, StackingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import warnings\n",
    "import Feature_Creation\n",
    "from Feature_Creation import create_features\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run Feature_Creation.ipynb\n",
    "#%run Auxillary_Functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_prediction_analysis(filename,CV= 5,verbose=False,do_forest=False,do_rf =True,do_logreg =True,do_svm=True,do_xgb=True,do_stacking=True):\n",
    "    ## Read data\n",
    "    data = pd.read_csv(filename)\n",
    "    data_feat = create_features(data)\n",
    "    if verbose==True:\n",
    "        print(data_feat.head())\n",
    "    \n",
    "    \n",
    "    RF_dict = {}\n",
    "    logreg_poly_dict = {}\n",
    "    SVM_poly_dict = {}\n",
    "    XGB_dict = {}\n",
    "    ### Normalizing Features and creating test train split and time series cross-validation\n",
    "    y = data_feat['target'].astype(int)\n",
    "    X = data_feat.drop(['Date','Adj Close','High','Low','Close','target'],axis=1)\n",
    "    \n",
    "    ### Continuous features\n",
    "    continuous = ['1day_pct', '2day_pct', '3day_pct', '4day_pct', '5day_pct', '7day_pct',\n",
    "                  '1day_pct_cs','ewma_7', 'ewma_50', 'ewma_200', 'RSI', 'MACD','Volume','day_var','open_close','open_prev_close','high_close']\n",
    "    ### Discrete features\n",
    "    discrete = ['prev_hot_streak','prev_cold_streak', 'current_hot_streak', 'current_cold_streak',\n",
    "                'RSI_overbought','RSI_oversold',\n",
    "                #'7g(50&200)','7l(50&200)','7g50','7g200',\n",
    "                'prev_current_hot', 'prev_current_cold','current_hot_prev_cold','current_cold_prev_hot',\n",
    "                'Month_1', 'Month_2', 'Month_3', 'Month_4', 'Month_5', 'Month_6',\n",
    "                'Month_7', 'Month_8', 'Month_9', 'Month_10', 'Month_11', 'Month_12',\n",
    "                'dayowk_0', 'dayowk_1', 'dayowk_2', 'dayowk_3', 'dayowk_4',\n",
    "               ]\n",
    "    ### Scale continuos features\n",
    "    scaler = StandardScaler()\n",
    "    X_cont = pd.DataFrame(scaler.fit_transform(X[continuous]),columns=continuous)\n",
    "    \n",
    "    ### Discerete Features\n",
    "    X_disc = X[discrete]\n",
    "\n",
    "    X_cont.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    X_disc.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    ### Combining\n",
    "    X = pd.concat([X_cont,X_disc],axis=1)\n",
    "\n",
    "    if CV=='tscv':    \n",
    "        train_size = X.shape[0]*4//5\n",
    "        X_test = X.iloc[train_size:]\n",
    "        X_train = X.iloc[0:train_size]\n",
    "        y_test = y.iloc[train_size:]\n",
    "        y_train = y.iloc[0:train_size]\n",
    "\n",
    "        ### Time series cross-validation\n",
    "        tscv = TimeSeriesSplit(n_splits=4)\n",
    "    else:\n",
    "        #print('here')\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    ### Naive Prediction\n",
    "    y_hat_test_naive = np.ones(len(y_test))\n",
    "    naive_dict = {'model':'Naive','precision':precision_score(y_hat_test_naive,y_test),\n",
    "                    'recall':recall_score(y_hat_test_naive,y_test),\n",
    "                    'accuracy':accuracy_score(y_hat_test_naive,y_test),\n",
    "                    'f1':f1_score(y_hat_test_naive,y_test),\n",
    "                 'return':np.nansum((np.array(X_test['1day_pct'].shift(-1))*(np.array(y_hat_test_naive))))}\n",
    "    if verbose==True:\n",
    "        print('naive return:',np.nansum((np.array(X_test['1day_pct'].shift(-1))*(np.array(y_hat_test_naive)))))\n",
    "\n",
    "    ### Large Forest\n",
    "    if do_forest == True:\n",
    "            forest = RandomForestClassifier(n_estimators=3000, max_depth= 10)\n",
    "            forest.fit(X_train, y_train)\n",
    "            #plot_feature_importances(forest,n_features=50)\n",
    "    \n",
    "    \n",
    "        \n",
    "    if do_rf == True:\n",
    "        rf_clf = RandomForestClassifier()\n",
    "        rf_param_grid = {\n",
    "            'n_estimators': [100],\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'max_depth': [None, 2, 3, 5,10],\n",
    "            'min_samples_split': [5,10,15],\n",
    "            'min_samples_leaf': [3,5,9,13]\n",
    "        }\n",
    "        rf_grid_search = GridSearchCV(rf_clf, rf_param_grid, cv=CV)\n",
    "        rf_grid_search.fit(X_train, y_train)\n",
    "        if verbose==True:\n",
    "            print(f\"Training Accuracy: {rf_grid_search.best_score_ :.2%}\")\n",
    "            print(\"\")\n",
    "            print(f\"Optimal Parameters: {rf_grid_search.best_params_}\")\n",
    "        best_rf = rf_grid_search.best_params_\n",
    "\n",
    "        y_hat_test_RF = rf_grid_search.predict(X_test)\n",
    "        RF_dict = {'model':'RF','precision':precision_score(y_hat_test_RF,y_test),'recall':recall_score(y_hat_test_RF,y_test),\n",
    "           'accuracy':accuracy_score(y_hat_test_RF,y_test),'f1':f1_score(y_hat_test_RF,y_test),\n",
    "           'return':np.nansum((np.array(X_test['1day_pct'].shift(-1))*((np.array(y_hat_test_RF)-1/2)*2))),\n",
    "            'information gain':accuracy_score(y_hat_test_RF,y_test)-accuracy_score(y_hat_test_naive,y_test)}\n",
    "        if verbose==True:\n",
    "            print(RF_dict)\n",
    "    \n",
    "    ### Logistic Regression\n",
    "    if do_logreg == True:\n",
    "        logreg_clf = LogisticRegression()\n",
    "        logreg_param_grid = {\n",
    "            'fit_intercept': [True,False],\n",
    "            'solver':['liblinear'],\n",
    "            'C': np.logspace(0,4,5),\n",
    "            'penalty': ['l2'],\n",
    "        }\n",
    "        logreg_grid_search = GridSearchCV(logreg_clf, logreg_param_grid, cv=CV)\n",
    "        logreg_grid_search.fit(X_train, y_train)\n",
    "        y_hat_test_log = logreg_grid_search.predict(X_test)\n",
    "        logreg_poly_dict = {'model':'Logistic','precision':precision_score(y_hat_test_log,y_test),\n",
    "                    'recall':recall_score(y_hat_test_log,y_test),\n",
    "                   'accuracy':accuracy_score(y_hat_test_log,y_test),\n",
    "                    'f1':f1_score(y_hat_test_log,y_test),\n",
    "                   'return':np.nansum((np.array(X_test['1day_pct'].shift(-1))*((np.array(y_hat_test_log)-1/2)*2))),\n",
    "                    'information gain':accuracy_score(y_hat_test_log,y_test)-accuracy_score(y_hat_test_naive,y_test)}\n",
    "        if verbose==True:\n",
    "            print(logreg_poly_dict)\n",
    "        \n",
    "    \n",
    "    ### SVM poly\n",
    "    if do_svm==True:\n",
    "        svm_clf_poly = svm.SVC(kernel='poly')\n",
    "        r_range =  np.array([0.25,0.5, 1,2,4])\n",
    "        gamma_range =  np.array([0.0001,0.001, 0.01,0.1])\n",
    "        d_range = np.array([2,3, 4])\n",
    "        param_grid = dict(gamma=gamma_range, degree=d_range, coef0=r_range)\n",
    "        svm_grid_search_poly = GridSearchCV(svm_clf_poly, param_grid, cv=CV)\n",
    "        svm_grid_search_poly.fit(X_train, y_train)\n",
    "        best_svm = svm_grid_search_poly.best_params_\n",
    "        y_hat_test_svm_poly = svm_grid_search_poly.predict(X_test)\n",
    "        SVM_poly_dict = {'model':'SVM_poly','precision':precision_score(y_hat_test_svm_poly,y_test),'recall':recall_score(y_hat_test_svm_poly,y_test),\n",
    "               'accuracy':accuracy_score(y_hat_test_svm_poly,y_test),'f1':f1_score(y_hat_test_svm_poly,y_test),\n",
    "                'return':np.nansum((np.array(X_test['1day_pct'].shift(-1))*((np.array(y_hat_test_svm_poly)-1/2)*2))),\n",
    "                'information gain':accuracy_score(y_hat_test_svm_poly,y_test)-accuracy_score(y_hat_test_naive,y_test)}\n",
    "        if verbose==True:\n",
    "            print(SVM_poly_dict)\n",
    "    \n",
    "    ##XGB\n",
    "    if do_xgb==True:\n",
    "        estimator = XGBClassifier(\n",
    "        objective= 'binary:logistic',\n",
    "        nthread=2,\n",
    "        seed=42)\n",
    "        parameters = {\n",
    "            'max_depth': range (2, 10, 1),\n",
    "            'n_estimators': range(20, 120, 10),\n",
    "            'learning_rate': [0.001,0.003,0.01, 0.03, 0.1]\n",
    "        }\n",
    "\n",
    "        xgb_grid_search = GridSearchCV(\n",
    "            estimator=estimator,\n",
    "            param_grid=parameters,\n",
    "            n_jobs = 10,\n",
    "            cv = CV,\n",
    "            verbose=False\n",
    "        )\n",
    "        xgb_grid_search.fit(X_train, y_train)\n",
    "        if verbose==True:\n",
    "            print(f\"Training Accuracy: {xgb_grid_search.best_score_ :.2%}\")\n",
    "            print(\"\")\n",
    "            print(f\"Optimal Parameters: {xgb_grid_search.best_params_}\")\n",
    "        xgb_best = xgb_grid_search.best_params_\n",
    "        y_hat_test_XGB = xgb_grid_search.predict(X_test)\n",
    "        XGB_dict = {'model':'XGB','precision':precision_score(y_hat_test_XGB,y_test),'recall':recall_score(y_hat_test_XGB,y_test),\n",
    "                   'accuracy':accuracy_score(y_hat_test_XGB,y_test),'f1':f1_score(y_hat_test_XGB,y_test),\n",
    "                   'return':np.nansum((np.array(X_test['1day_pct'].shift(-1))*((np.array(y_hat_test_XGB)-1/2)*2))),\n",
    "                   'information gain':accuracy_score(y_hat_test_XGB,y_test)-accuracy_score(y_hat_test_naive,y_test)}\n",
    "        if verbose==True:\n",
    "            print(XGB_dict)\n",
    "        \n",
    "    \n",
    "    if do_stacking==True:\n",
    "        rf_base = RandomForestClassifier(n_estimators = best_rf['n_estimators'],\n",
    "                                               criterion = best_rf['criterion'],max_depth=best_rf['max_depth'],\n",
    "                                               min_samples_split = best_rf['min_samples_split'],\n",
    "                                              min_samples_leaf= best_rf['min_samples_leaf'])\n",
    "        xgb_base = XGBClassifier(n_estimators = xgb_best['n_estimators'],\n",
    "                                 max_depth = xgb_best['max_depth'],\n",
    "                                 learning_rate = xgb_best['learning_rate'],\n",
    "                                objective= 'binary:logistic',\n",
    "                                nthread=2,\n",
    "                                seed=42)\n",
    "        base_models = [('random_forest', rf_base),\n",
    "               ('xgb', xgb_base)]          \n",
    "        stack_clf = StackingClassifier(estimators = base_models,final_estimator = LogisticRegression(),\n",
    "                                           cv = 5)\n",
    "        stack_clf.fit(X_train, y_train)\n",
    "        y_hat_test_stack = stack_clf.predict(X_test)\n",
    "        stack_dict = {'model':'stack','precision':precision_score(y_hat_test_stack,y_test),'recall':recall_score(y_hat_test_stack,y_test),\n",
    "                   'accuracy':accuracy_score(y_hat_test_stack,y_test),'f1':f1_score(y_hat_test_stack,y_test),\n",
    "                   'return':np.nansum((np.array(X_test['1day_pct'].shift(-1))*((np.array(y_hat_test_stack)-1/2)*2))),\n",
    "                     'information gain':accuracy_score(y_hat_test_stack,y_test)-accuracy_score(y_hat_test_naive,y_test)}\n",
    "        if verbose==True:\n",
    "            print(stack_dict)\n",
    "    return naive_dict, RF_dict, logreg_poly_dict, SVM_poly_dict, XGB_dict, stack_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
