{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, StackingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import warnings\n",
    "import Feature_Creation\n",
    "from Feature_Creation import create_features\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_frame(target_ETF, horizon = 1):\n",
    "    ##Import dates\n",
    "    frame_10 = pd.read_csv(os.path.join(os.path.abspath(os.getcwd()),'data','10_ETF','SPY.csv'),usecols=['Date'])\n",
    "    # Check if defined horizon is in approved list\n",
    "    horizon_list = [1,2,3,5,10,20,40,60,120,250]\n",
    "    if horizon not in horizon_list:\n",
    "        raise ValueError(\"horizon must be one of [1,2,3,5,10,20,40,60,120,250]\")\n",
    "    \n",
    "    frame_10['Date'] = pd.to_datetime(frame_10['Date'])\n",
    "    frame_10['Month'] = frame_10['Date'].dt.month\n",
    "    frame_10['dayowk'] = frame_10['Date'].dt.dayofweek\n",
    "    frame_10 = pd.get_dummies(data = frame_10,columns = ['Month','dayowk'])\n",
    "    frame_10.drop(['Date'],axis=1,inplace=True)\n",
    "    \n",
    "    ##Joining all the ETF's together\n",
    "    for etf in ['SPY','IWM','EEM','TLT','LQD','TIP','IYR','GLD','OIH','FXE']:\n",
    "        frame = pd.read_csv(os.path.join(os.path.abspath(os.getcwd()),'data','10_ETF',etf+'.csv'),usecols=['Volume','Adj Close'])\n",
    "#        frame.rename(columns={'Volume':etf+'_volume'}, inplace=True)\n",
    "        if horizon == 1:\n",
    "            frame[etf+'_h_ret'] = (frame['Adj Close']/frame['Adj Close'].shift(1)) -1\n",
    "            frame[etf+'volume'] = frame['Volume']\n",
    "        else:\n",
    "            frame[etf+'_h_ret'] = (frame['Adj Close']/frame['Adj Close'].shift(horizon)) -1\n",
    "            lagged =  horizon_list[horizon_list.index(horizon)-1]\n",
    "            for j in range(1,lagged+1):\n",
    "                frame[etf +'_'+ str(j)+'_lag_ret'] = frame[etf+'_h_ret'].shift(j)#(frame['Adj Close'].shift(j)/frame['Adj Close'].shift(horizon+j)) -1\n",
    "            frame[etf+'_h_vol'] = frame['Volume'].rolling(horizon).mean()\n",
    "            for j in range(1,lagged+1):\n",
    "                frame[etf +'_'+ str(j)+'_lag_vol'] = frame[etf+'_h_vol'].shift(j)\n",
    "        if etf==target_ETF:\n",
    "            frame['target'] = frame['Adj Close'] <= frame['Adj Close'].shift(-horizon)\n",
    "        frame.drop(['Adj Close'],axis=1,inplace=True)\n",
    "        frame.drop(['Volume'],axis=1,inplace=True)\n",
    "        frame_10 = pd.concat([frame_10, frame],axis=1) \n",
    "    return frame_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_frame_rand(target_ETF, horizon = 1):\n",
    "    ##Import dates\n",
    "    frame_10 = pd.read_csv(os.path.join(os.path.abspath(os.getcwd()),'data','10_ETF','SPY.csv'),usecols=['Date'])\n",
    "    # Check if defined horizon is in approved list\n",
    "    horizon_list = [1,2,3,5,10,20,40,60,120,250]\n",
    "    if horizon not in horizon_list:\n",
    "        raise ValueError(\"horizon must be one of [1,2,3,5,10,20,40,60,120,250]\")\n",
    "    \n",
    "    frame_10['Date'] = pd.to_datetime(frame_10['Date'])\n",
    "    frame_10['Month'] = frame_10['Date'].dt.month\n",
    "    frame_10['dayowk'] = frame_10['Date'].dt.dayofweek\n",
    "    frame_10 = pd.get_dummies(data = frame_10,columns = ['Month','dayowk'])\n",
    "    frame_10.drop(['Date'],axis=1,inplace=True)\n",
    "    \n",
    "    ##Joining all the ETF's together\n",
    "    for etf in ['random1','random2','random3','random4','random5']:\n",
    "        frame = pd.read_csv(os.path.join(os.path.abspath(os.getcwd()),'data','10_ETF',etf+'.csv'),usecols=['Volume','Adj Close'])\n",
    "#        frame.rename(columns={'Volume':etf+'_volume'}, inplace=True)\n",
    "        if horizon == 1:\n",
    "            frame[etf+'_h_ret'] = (frame['Adj Close']/frame['Adj Close'].shift(1)) -1\n",
    "            frame[etf+'volume'] = frame['Volume']\n",
    "        else:\n",
    "            frame[etf+'_h_ret'] = (frame['Adj Close']/frame['Adj Close'].shift(horizon)) -1\n",
    "            lagged =  horizon_list[horizon_list.index(horizon)-1]\n",
    "            for j in range(1,lagged+1):\n",
    "                frame[etf +'_'+ str(j)+'_lag_ret'] = frame[etf+'_h_ret'].shift(j)#(frame['Adj Close'].shift(j)/frame['Adj Close'].shift(horizon+j)) -1\n",
    "            frame[etf+'_h_vol'] = frame['Volume'].rolling(horizon).mean()\n",
    "            for j in range(1,lagged+1):\n",
    "                frame[etf +'_'+ str(j)+'_lag_vol'] = frame[etf+'_h_vol'].shift(j)\n",
    "        if etf==target_ETF:\n",
    "            frame['target'] = frame['Adj Close'] <= frame['Adj Close'].shift(-horizon)\n",
    "        frame.drop(['Adj Close'],axis=1,inplace=True)\n",
    "        frame.drop(['Volume'],axis=1,inplace=True)\n",
    "        frame_10 = pd.concat([frame_10, frame],axis=1) \n",
    "    return frame_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def liew_mayster(data_feat,CV= 5,verbose=False,do_forest=False,do_rf =True,do_logreg =True,do_svm=True,do_xgb=True,do_stacking=True):\n",
    "    beginning = datetime.datetime.now()\n",
    "    data_feat.dropna(inplace=True)\n",
    "    if verbose==True:\n",
    "        print(data_feat.head())\n",
    "    \n",
    "    \n",
    "    RF_dict = {}\n",
    "    logreg_poly_dict = {}\n",
    "    SVM_poly_dict = {}\n",
    "    XGB_dict = {}\n",
    "    ### Normalizing Features and creating test train split and time series cross-validation\n",
    "    y = data_feat['target'].astype(int)\n",
    "    X = data_feat.drop(['target'],axis=1)\n",
    "    #X.dropna(inplace=True)\n",
    "    \n",
    "    ### Continuous features\n",
    "    continuous = X.columns[X.nunique()>=3]\n",
    "    ### Discrete features\n",
    "    discrete = X.columns[X.nunique()< 3]\n",
    "    ### Scale continuos features\n",
    "    scaler = StandardScaler()\n",
    "    X_cont = pd.DataFrame(scaler.fit_transform(X[continuous]),columns=continuous)\n",
    "    \n",
    "    ### Discerete Features\n",
    "    X_disc = X[discrete]\n",
    "\n",
    "    X_cont.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    X_disc.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    ### Combining\n",
    "    X = pd.concat([X_cont,X_disc],axis=1)\n",
    "\n",
    "    if CV=='tscv':    \n",
    "        train_size = X.shape[0]*4//5\n",
    "        X_test = X.iloc[train_size:]\n",
    "        X_train = X.iloc[0:train_size]\n",
    "        y_test = y.iloc[train_size:]\n",
    "        y_train = y.iloc[0:train_size]\n",
    "\n",
    "        ### Time series cross-validation\n",
    "        tscv = TimeSeriesSplit(n_splits=4)\n",
    "    else:\n",
    "        #print('here')\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    ### Naive Prediction\n",
    "    y_hat_test_naive = np.ones(len(y_test))\n",
    "    naive_dict = {'model':'Naive','precision':precision_score(y_hat_test_naive,y_test),\n",
    "                    'recall':recall_score(y_hat_test_naive,y_test),\n",
    "                    'accuracy':accuracy_score(y_hat_test_naive,y_test),\n",
    "                    'f1':f1_score(y_hat_test_naive,y_test)}\n",
    "                 #'return':np.nansum((np.array(X_test['1day_pct'].shift(-1))*(np.array(y_hat_test_naive))))}\n",
    "    #if verbose==True:\n",
    "     #   print('naive return:',np.nansum((np.array(X_test['1day_pct'].shift(-1))*(np.array(y_hat_test_naive)))))\n",
    "\n",
    "    ### Large Forest\n",
    "    if do_forest == True:\n",
    "            forest = RandomForestClassifier(n_estimators=3000, max_depth= 10)\n",
    "            forest.fit(X_train, y_train)\n",
    "            #plot_feature_importances(forest,n_features=50)\n",
    "    \n",
    "    print(accuracy_score(y_hat_test_naive,y_test))\n",
    "    start = datetime.datetime.now()    \n",
    "    if do_rf == True:\n",
    "        rf_clf = RandomForestClassifier()\n",
    "        rf_param_grid = {\n",
    "            'n_estimators': [100],\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'max_depth': [None, 2, 3, 5,10],\n",
    "            'min_samples_split': [5,10,15],\n",
    "            'min_samples_leaf': [3,5,9,13]\n",
    "        }\n",
    "        rf_grid_search = GridSearchCV(rf_clf, rf_param_grid, cv=CV,n_jobs=-1)\n",
    "        rf_grid_search.fit(X_train, y_train)\n",
    "        if verbose==True:\n",
    "            print(f\"Training Accuracy: {rf_grid_search.best_score_ :.2%}\")\n",
    "            print(\"\")\n",
    "            print(f\"Optimal Parameters: {rf_grid_search.best_params_}\")\n",
    "        best_rf = rf_grid_search.best_params_\n",
    "\n",
    "        y_hat_test_RF = rf_grid_search.predict(X_test)\n",
    "        RF_dict = {'model':'RF','precision':precision_score(y_hat_test_RF,y_test),'recall':recall_score(y_hat_test_RF,y_test),\n",
    "           'accuracy':accuracy_score(y_hat_test_RF,y_test),'f1':f1_score(y_hat_test_RF,y_test),\n",
    "           #'return':np.nansum((np.array(X_test['1day_pct'].shift(-1))*((np.array(y_hat_test_RF)-1/2)*2))),\n",
    "            'information gain':accuracy_score(y_hat_test_RF,y_test)-accuracy_score(y_hat_test_naive,y_test)}\n",
    "        if verbose==True:\n",
    "            print(RF_dict)\n",
    "    end = datetime.datetime.now()\n",
    "    print('RF time', end-start)\n",
    "    \n",
    "    start = datetime.datetime.now()\n",
    "    ### Logistic Regression\n",
    "    if do_logreg == True:\n",
    "        logreg_clf = LogisticRegression()\n",
    "        logreg_param_grid = {\n",
    "            'fit_intercept': [True,False],\n",
    "            'solver':['liblinear'],\n",
    "            'C': np.logspace(0,4,5),\n",
    "            'penalty': ['l2'],\n",
    "        }\n",
    "        logreg_grid_search = GridSearchCV(logreg_clf, logreg_param_grid, cv=CV,n_jobs=-1)\n",
    "        logreg_grid_search.fit(X_train, y_train)\n",
    "        y_hat_test_log = logreg_grid_search.predict(X_test)\n",
    "        logreg_poly_dict = {'model':'Logistic','precision':precision_score(y_hat_test_log,y_test),\n",
    "                    'recall':recall_score(y_hat_test_log,y_test),\n",
    "                   'accuracy':accuracy_score(y_hat_test_log,y_test),\n",
    "                    'f1':f1_score(y_hat_test_log,y_test),\n",
    "                   #'return':np.nansum((np.array(X_test['1day_pct'].shift(-1))*((np.array(y_hat_test_log)-1/2)*2))),\n",
    "                    'information gain':accuracy_score(y_hat_test_log,y_test)-accuracy_score(y_hat_test_naive,y_test)}\n",
    "        if verbose==True:\n",
    "            print(logreg_poly_dict)\n",
    "    end = datetime.datetime.now()    \n",
    "    print('LOG time', end-start)\n",
    "    \n",
    "    start = datetime.datetime.now()\n",
    "    ### SVM poly\n",
    "    if do_svm==True:\n",
    "        svm_clf_poly = svm.SVC(kernel='poly')\n",
    "        r_range =  np.array([0.25,0.5, 1,2,4])\n",
    "        gamma_range =  np.array([0.0001,0.001, 0.01,0.1])\n",
    "        d_range = np.array([2,3, 4])\n",
    "        param_grid = dict(gamma=gamma_range, degree=d_range, coef0=r_range)\n",
    "        svm_grid_search_poly = GridSearchCV(svm_clf_poly, param_grid, cv=CV,n_jobs=-1)\n",
    "        svm_grid_search_poly.fit(X_train, y_train)\n",
    "        best_svm = svm_grid_search_poly.best_params_\n",
    "        y_hat_test_svm_poly = svm_grid_search_poly.predict(X_test)\n",
    "        SVM_poly_dict = {'model':'SVM_poly','precision':precision_score(y_hat_test_svm_poly,y_test),'recall':recall_score(y_hat_test_svm_poly,y_test),\n",
    "               'accuracy':accuracy_score(y_hat_test_svm_poly,y_test),'f1':f1_score(y_hat_test_svm_poly,y_test),\n",
    "                #'return':np.nansum((np.array(X_test['1day_pct'].shift(-1))*((np.array(y_hat_test_svm_poly)-1/2)*2))),\n",
    "                'information gain':accuracy_score(y_hat_test_svm_poly,y_test)-accuracy_score(y_hat_test_naive,y_test)}\n",
    "        if verbose==True:\n",
    "            print(SVM_poly_dict)\n",
    "    end = datetime.datetime.now()\n",
    "    print('SVM time', end-start)\n",
    "    \n",
    "    start = datetime.datetime.now()\n",
    "    ##XGB\n",
    "    if do_xgb==True:\n",
    "        estimator = XGBClassifier(\n",
    "        objective= 'binary:logistic',\n",
    "        nthread=2,\n",
    "        seed=42)\n",
    "        parameters = {\n",
    "            'max_depth': range (2, 10, 2),\n",
    "            'n_estimators': range(20, 120, 20),\n",
    "            'learning_rate': [0.001,0.003,0.01, 0.03, 0.1]\n",
    "        }\n",
    "\n",
    "        xgb_grid_search = GridSearchCV(\n",
    "            estimator=estimator,\n",
    "            param_grid=parameters,\n",
    "            n_jobs = -1,\n",
    "            cv = CV,\n",
    "            verbose=False\n",
    "        )\n",
    "        xgb_grid_search.fit(X_train, y_train)\n",
    "        if verbose==True:\n",
    "            print(f\"Training Accuracy: {xgb_grid_search.best_score_ :.2%}\")\n",
    "            print(\"\")\n",
    "            print(f\"Optimal Parameters: {xgb_grid_search.best_params_}\")\n",
    "        xgb_best = xgb_grid_search.best_params_\n",
    "        y_hat_test_XGB = xgb_grid_search.predict(X_test)\n",
    "        XGB_dict = {'model':'XGB','precision':precision_score(y_hat_test_XGB,y_test),'recall':recall_score(y_hat_test_XGB,y_test),\n",
    "                   'accuracy':accuracy_score(y_hat_test_XGB,y_test),'f1':f1_score(y_hat_test_XGB,y_test),\n",
    "                   #'return':np.nansum((np.array(X_test['1day_pct'].shift(-1))*((np.array(y_hat_test_XGB)-1/2)*2))),\n",
    "                   'information gain':accuracy_score(y_hat_test_XGB,y_test)-accuracy_score(y_hat_test_naive,y_test)}\n",
    "        if verbose==True:\n",
    "            print(XGB_dict)\n",
    "    end = datetime.datetime.now()    \n",
    "    print('XGB time', end-start)\n",
    "    \n",
    "    start = datetime.datetime.now()\n",
    "    if do_stacking==True:\n",
    "        rf_base = RandomForestClassifier(n_estimators = best_rf['n_estimators'],\n",
    "                                               criterion = best_rf['criterion'],max_depth=best_rf['max_depth'],\n",
    "                                               min_samples_split = best_rf['min_samples_split'],\n",
    "                                              min_samples_leaf= best_rf['min_samples_leaf'])\n",
    "        xgb_base = XGBClassifier(n_estimators = xgb_best['n_estimators'],\n",
    "                                 max_depth = xgb_best['max_depth'],\n",
    "                                 learning_rate = xgb_best['learning_rate'],\n",
    "                                objective= 'binary:logistic',\n",
    "                                nthread=2,\n",
    "                                seed=42)\n",
    "        base_models = [('random_forest', rf_base),\n",
    "               ('xgb', xgb_base)]          \n",
    "        stack_clf = StackingClassifier(estimators = base_models,final_estimator = LogisticRegression(),\n",
    "                                           cv = 5)\n",
    "        stack_clf.fit(X_train, y_train)\n",
    "        y_hat_test_stack = stack_clf.predict(X_test)\n",
    "        stack_dict = {'model':'stack','precision':precision_score(y_hat_test_stack,y_test),'recall':recall_score(y_hat_test_stack,y_test),\n",
    "                   'accuracy':accuracy_score(y_hat_test_stack,y_test),'f1':f1_score(y_hat_test_stack,y_test),\n",
    "                   #'return':np.nansum((np.array(X_test['1day_pct'].shift(-1))*((np.array(y_hat_test_stack)-1/2)*2))),\n",
    "                     'information gain':accuracy_score(y_hat_test_stack,y_test)-accuracy_score(y_hat_test_naive,y_test)}\n",
    "        if verbose==True:\n",
    "            print(stack_dict)\n",
    "    end = datetime.datetime.now()\n",
    "    print('stack time', end-start)\n",
    "    print('Total',end-beginning)\n",
    "    return naive_dict, RF_dict, logreg_poly_dict, SVM_poly_dict, XGB_dict, stack_dict\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Month_1  Month_2  Month_3  Month_4  Month_5  Month_6  Month_7  Month_8  \\\n",
      "8         1        0        0        0        0        0        0        0   \n",
      "9         1        0        0        0        0        0        0        0   \n",
      "10        1        0        0        0        0        0        0        0   \n",
      "11        1        0        0        0        0        0        0        0   \n",
      "12        1        0        0        0        0        0        0        0   \n",
      "\n",
      "    Month_9  Month_10  ...  OIH_2_lag_vol  OIH_3_lag_vol  FXE_h_ret  \\\n",
      "8         0         0  ...       467160.0       454240.0  -0.010925   \n",
      "9         0         0  ...       438380.0       467160.0  -0.003691   \n",
      "10        0         0  ...       421760.0       438380.0  -0.014473   \n",
      "11        0         0  ...       411300.0       421760.0  -0.023154   \n",
      "12        0         0  ...       441040.0       411300.0  -0.024028   \n",
      "\n",
      "    FXE_1_lag_ret  FXE_2_lag_ret  FXE_3_lag_ret  FXE_h_vol  FXE_1_lag_vol  \\\n",
      "8       -0.008593      -0.013452      -0.025684   698060.0       637440.0   \n",
      "9       -0.010925      -0.008593      -0.013452   712120.0       698060.0   \n",
      "10      -0.003691      -0.010925      -0.008593   964020.0       712120.0   \n",
      "11      -0.014473      -0.003691      -0.010925  1351420.0       964020.0   \n",
      "12      -0.023154      -0.014473      -0.003691  1545620.0      1351420.0   \n",
      "\n",
      "    FXE_2_lag_vol  FXE_3_lag_vol  \n",
      "8        643060.0       628680.0  \n",
      "9        637440.0       643060.0  \n",
      "10       698060.0       637440.0  \n",
      "11       712120.0       698060.0  \n",
      "12       964020.0       712120.0  \n",
      "\n",
      "[5 rows x 97 columns]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2894\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'target'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-19b99d4c8578>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mliew_mayster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'random1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhorizon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-aa3f134e6dc8>\u001b[0m in \u001b[0;36mliew_mayster\u001b[0;34m(data_feat, CV, verbose, do_forest, do_rf, do_logreg, do_svm, do_xgb, do_stacking)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mXGB_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m### Normalizing Features and creating test train split and time series cross-validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_feat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_feat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#X.dropna(inplace=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2902\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2903\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2895\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'target'"
     ]
    }
   ],
   "source": [
    "liew_mayster(create_frame('random1',horizon=5),verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
